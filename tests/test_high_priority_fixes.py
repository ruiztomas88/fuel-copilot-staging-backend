"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            ðŸ§ª HIGH PRIORITY FIXES TESTS - December 2025                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Tests for HIGH priority bug fixes:                                            â•‘
â•‘  1. Driver scorecard filter: total_records > 50 (not total_miles > 1)         â•‘
â•‘  2. JSON sanitizer for inf/nan values in cost_router                          â•‘
â•‘  3. Fleet Score calculation edge cases                                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pytest
import json
import math
from unittest.mock import patch, MagicMock


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST 1: Driver Scorecard Filter (total_records > 50)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class TestDriverScorecardFilter:
    """
    Tests for driver scorecard filtering logic.

    BUG: Drivers were being filtered out because total_miles was 0 even though
    they had activity (records). This happened because odom_delta_mi can be null.

    FIX: Changed filter from `total_miles > 1` to `total_records > 50`
    """

    # Sample driver data simulating database results
    DRIVER_WITH_MILES = {
        "truck_id": "TRK-001",
        "overall_score": 75.0,
        "grade": "B",
        "scores": {
            "speed_optimization": 80,
            "rpm_discipline": 70,
            "idle_management": 75,
            "fuel_consistency": 72,
            "mpg_performance": 78,
        },
        "metrics": {
            "avg_speed_mph": 58.0,
            "max_speed_mph": 72.0,
            "avg_rpm": 1400,
            "idle_pct": 12.5,
            "avg_mpg": 6.2,
            "best_mpg": 7.1,
            "worst_mpg": 5.5,
            "total_miles": 450.5,
            "total_records": 200,
        },
    }

    DRIVER_NO_MILES_BUT_RECORDS = {
        "truck_id": "TRK-002",
        "overall_score": 68.0,
        "grade": "C",
        "scores": {
            "speed_optimization": 65,
            "rpm_discipline": 72,
            "idle_management": 68,
            "fuel_consistency": 70,
            "mpg_performance": 65,
        },
        "metrics": {
            "avg_speed_mph": 55.0,
            "max_speed_mph": 68.0,
            "avg_rpm": 1350,
            "idle_pct": 15.0,
            "avg_mpg": 5.8,
            "best_mpg": 6.5,
            "worst_mpg": 5.2,
            "total_miles": 0.0,  # No miles (odom_delta_mi was null)
            "total_records": 150,  # But has activity!
        },
    }

    DRIVER_NO_ACTIVITY = {
        "truck_id": "TRK-003",
        "overall_score": 45.0,
        "grade": "D",
        "scores": {
            "speed_optimization": 40,
            "rpm_discipline": 50,
            "idle_management": 45,
            "fuel_consistency": 48,
            "mpg_performance": 42,
        },
        "metrics": {
            "avg_speed_mph": 0.0,
            "max_speed_mph": 0.0,
            "avg_rpm": 0,
            "idle_pct": 0.0,
            "avg_mpg": 0.0,
            "best_mpg": 0.0,
            "worst_mpg": 0.0,
            "total_miles": 0.0,
            "total_records": 10,  # Too few records
        },
    }

    def test_filter_includes_driver_with_miles_and_records(self):
        """Driver with miles and records should be included"""
        drivers = [self.DRIVER_WITH_MILES.copy()]

        # Apply new filter logic
        filtered = [d for d in drivers if d["metrics"].get("total_records", 0) > 50]

        assert len(filtered) == 1
        assert filtered[0]["truck_id"] == "TRK-001"

    def test_filter_includes_driver_without_miles_but_with_records(self):
        """
        FIX: Driver without miles but with records should NOW be included
        (Old behavior would exclude this driver)
        """
        drivers = [self.DRIVER_NO_MILES_BUT_RECORDS.copy()]

        # Old filter (would exclude)
        old_filtered = [d for d in drivers if d["metrics"]["total_miles"] > 1]
        assert len(old_filtered) == 0, "Old filter excludes driver with 0 miles"

        # New filter (includes)
        new_filtered = [d for d in drivers if d["metrics"].get("total_records", 0) > 50]
        assert len(new_filtered) == 1, "New filter includes driver with 150 records"
        assert new_filtered[0]["truck_id"] == "TRK-002"

    def test_filter_excludes_driver_with_no_activity(self):
        """Driver with too few records should be excluded"""
        drivers = [self.DRIVER_NO_ACTIVITY.copy()]

        filtered = [d for d in drivers if d["metrics"].get("total_records", 0) > 50]

        assert len(filtered) == 0

    def test_filter_mixed_fleet(self):
        """Test with mixed fleet - should include 2 of 3 drivers"""
        drivers = [
            self.DRIVER_WITH_MILES.copy(),
            self.DRIVER_NO_MILES_BUT_RECORDS.copy(),
            self.DRIVER_NO_ACTIVITY.copy(),
        ]

        filtered = [d for d in drivers if d["metrics"].get("total_records", 0) > 50]

        assert len(filtered) == 2
        truck_ids = [d["truck_id"] for d in filtered]
        assert "TRK-001" in truck_ids
        assert "TRK-002" in truck_ids
        assert "TRK-003" not in truck_ids

    def test_total_records_field_exists(self):
        """Verify total_records is included in driver metrics"""
        # This tests that the new field is being populated
        driver = self.DRIVER_WITH_MILES.copy()

        assert "total_records" in driver["metrics"]
        assert isinstance(driver["metrics"]["total_records"], int)
        assert driver["metrics"]["total_records"] > 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST 2: JSON Sanitizer for inf/nan Values
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class TestJSONSanitizer:
    """
    Tests for JSON sanitization to prevent serialization errors.

    BUG: API was returning 500 errors because float('inf') and float('nan')
    cannot be serialized to JSON.

    FIX: Added sanitize_json() function that replaces inf/nan with 0.0
    (Using 0.0 instead of None to maintain numeric type consistency)
    """

    def test_sanitize_json_handles_inf(self):
        """Should replace infinity with 0.0"""
        from routers.cost_router import sanitize_json

        data = {"cost": float("inf"), "name": "test"}
        result = sanitize_json(data)

        assert result["cost"] == 0.0
        assert result["name"] == "test"

    def test_sanitize_json_handles_negative_inf(self):
        """Should replace negative infinity with 0.0"""
        from routers.cost_router import sanitize_json

        data = {"value": float("-inf")}
        result = sanitize_json(data)

        assert result["value"] == 0.0

    def test_sanitize_json_handles_nan(self):
        """Should replace NaN with 0.0"""
        from routers.cost_router import sanitize_json

        data = {"mpg": float("nan"), "truck": "TRK-001"}
        result = sanitize_json(data)

        assert result["mpg"] == 0.0
        assert result["truck"] == "TRK-001"

    def test_sanitize_json_preserves_valid_floats(self):
        """Should preserve valid float values"""
        from routers.cost_router import sanitize_json

        data = {"cost": 2.45, "mpg": 6.5, "miles": 1234.56}
        result = sanitize_json(data)

        assert result["cost"] == 2.45
        assert result["mpg"] == 6.5
        assert result["miles"] == 1234.56

    def test_sanitize_json_handles_nested_dict(self):
        """Should sanitize nested dictionaries"""
        from routers.cost_router import sanitize_json

        data = {
            "fleet": {"cost": float("inf"), "trucks": {"avg_mpg": float("nan")}},
            "valid": 123.45,
        }
        result = sanitize_json(data)

        assert result["fleet"]["cost"] == 0.0
        assert result["fleet"]["trucks"]["avg_mpg"] == 0.0
        assert result["valid"] == 123.45

    def test_sanitize_json_handles_list(self):
        """Should sanitize values in lists"""
        from routers.cost_router import sanitize_json

        data = {"values": [1.0, float("inf"), 3.0, float("nan"), 5.0]}
        result = sanitize_json(data)

        assert result["values"] == [1.0, 0.0, 3.0, 0.0, 5.0]

    def test_sanitize_json_handles_list_of_dicts(self):
        """Should sanitize list of dictionaries"""
        from routers.cost_router import sanitize_json

        data = {
            "trucks": [
                {"id": "T1", "cost": 2.5},
                {"id": "T2", "cost": float("inf")},
                {"id": "T3", "cost": float("nan")},
            ]
        }
        result = sanitize_json(data)

        assert result["trucks"][0]["cost"] == 2.5
        assert result["trucks"][1]["cost"] == 0.0
        assert result["trucks"][2]["cost"] == 0.0

    def test_sanitize_json_preserves_none(self):
        """Should preserve None values"""
        from routers.cost_router import sanitize_json

        data = {"value": None, "other": "test"}
        result = sanitize_json(data)

        assert result["value"] is None
        assert result["other"] == "test"

    def test_sanitize_json_preserves_strings_and_ints(self):
        """Should preserve strings and integers"""
        from routers.cost_router import sanitize_json

        data = {"name": "Fleet 1", "count": 42, "active": True}
        result = sanitize_json(data)

        assert result["name"] == "Fleet 1"
        assert result["count"] == 42
        assert result["active"] is True

    def test_sanitized_json_is_serializable(self):
        """Sanitized data should be JSON serializable"""
        from routers.cost_router import sanitize_json

        data = {
            "cost": float("inf"),
            "mpg": float("nan"),
            "nested": {"value": float("-inf")},
            "list": [float("nan"), 1.0, float("inf")],
        }
        result = sanitize_json(data)

        # Should not raise
        json_str = json.dumps(result)
        assert isinstance(json_str, str)

        # Should be parseable
        parsed = json.loads(json_str)
        assert parsed["cost"] == 0.0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST 3: Fleet Score Calculation Edge Cases
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class TestFleetScoreCalculation:
    """
    Tests for Fleet Score calculation edge cases.

    BUG: When costPerMile = 0, the formula gave:
    costScore = 100 - ((0 - 2.26) / 2.26) * 100 = ~200 (should be capped)

    FIX:
    1. Cap costScore between 0 and 100
    2. When cost = 0, assign neutral score (50) instead of inflated score
    """

    BENCHMARK_COST = 2.26
    BENCHMARK_MPG = 6.5
    TARGET_UTILIZATION = 60  # Changed from 95% to 60%

    def calculate_fleet_score(
        self, cost_per_mile: float, utilization: float, avg_mpg: float
    ) -> int:
        """
        Replicate the frontend Fleet Score calculation with fixes.
        """
        # Cost score (lower is better, benchmark $2.26)
        if cost_per_mile > 0:
            cost_score = min(
                100,
                max(
                    0,
                    100
                    - ((cost_per_mile - self.BENCHMARK_COST) / self.BENCHMARK_COST)
                    * 100,
                ),
            )
        else:
            cost_score = 50  # Neutral score when no cost data

        # Utilization score (target 60%)
        util_score = min(100, utilization * (100 / self.TARGET_UTILIZATION))

        # MPG score (benchmark 6.5 MPG)
        if avg_mpg > 0:
            mpg_score = min(100, (avg_mpg / self.BENCHMARK_MPG) * 100)
        else:
            mpg_score = 50  # Neutral score when no MPG data

        # Weighted average
        return round((cost_score * 0.35) + (util_score * 0.35) + (mpg_score * 0.30))

    def test_score_with_valid_data(self):
        """Normal case with valid data"""
        score = self.calculate_fleet_score(
            cost_per_mile=2.50, utilization=45.0, avg_mpg=6.0
        )

        # Cost: 100 - ((2.50 - 2.26) / 2.26) * 100 â‰ˆ 89.4
        # Util: 45 * (100/60) = 75
        # MPG: (6.0 / 6.5) * 100 â‰ˆ 92.3
        # Score: 89.4*0.35 + 75*0.35 + 92.3*0.30 â‰ˆ 85

        assert 75 <= score <= 95, f"Expected score ~85, got {score}"

    def test_score_with_zero_cost(self):
        """
        FIX: When cost is 0, should NOT give inflated score
        Old behavior: score would be ~200 (uncapped)
        New behavior: costScore = 50 (neutral)
        """
        score = self.calculate_fleet_score(
            cost_per_mile=0, utilization=45.0, avg_mpg=6.0
        )

        # Cost: 50 (neutral)
        # Util: 75
        # MPG: 92.3
        # Score: 50*0.35 + 75*0.35 + 92.3*0.30 â‰ˆ 71

        assert 60 <= score <= 80, f"Score with 0 cost should be ~71, got {score}"
        assert score < 100, "Score should not be inflated to 100"

    def test_score_with_zero_mpg(self):
        """When MPG is 0, should assign neutral score"""
        score = self.calculate_fleet_score(
            cost_per_mile=2.26, utilization=60.0, avg_mpg=0
        )

        # Cost: 100 (exactly at benchmark)
        # Util: 100 (at target)
        # MPG: 50 (neutral)
        # Score: 100*0.35 + 100*0.35 + 50*0.30 = 85

        assert score == 85, f"Expected score 85, got {score}"

    def test_score_with_all_zeros(self):
        """When all values are 0, should get neutral score"""
        score = self.calculate_fleet_score(cost_per_mile=0, utilization=0, avg_mpg=0)

        # Cost: 50 (neutral)
        # Util: 0 (no activity)
        # MPG: 50 (neutral)
        # Score: 50*0.35 + 0*0.35 + 50*0.30 = 32.5

        assert 30 <= score <= 35, f"Expected score ~33, got {score}"

    def test_score_excellent_fleet(self):
        """Excellent fleet should score 85+"""
        score = self.calculate_fleet_score(
            cost_per_mile=1.80,  # Below benchmark
            utilization=70.0,  # Above target
            avg_mpg=7.0,  # Above benchmark
        )

        assert score >= 85, f"Excellent fleet should score 85+, got {score}"

    def test_score_poor_fleet(self):
        """Poor fleet should score below 55"""
        score = self.calculate_fleet_score(
            cost_per_mile=4.00,  # Way above benchmark
            utilization=20.0,  # Low utilization
            avg_mpg=4.5,  # Below benchmark
        )

        assert score < 55, f"Poor fleet should score below 55, got {score}"

    def test_cost_score_capped_at_100(self):
        """Cost score should be capped at 100 even with very low costs"""
        score = self.calculate_fleet_score(
            cost_per_mile=0.50, utilization=60.0, avg_mpg=6.5  # Very low cost
        )

        # Cost: min(100, 100 - ((0.50 - 2.26)/2.26)*100) = min(100, 177.8) = 100
        # Util: 100
        # MPG: 100
        # Score: 100*0.35 + 100*0.35 + 100*0.30 = 100

        assert score == 100, f"Expected max score 100, got {score}"

    def test_cost_score_capped_at_0(self):
        """Cost score should not go negative with very high costs"""
        score = self.calculate_fleet_score(
            cost_per_mile=10.00, utilization=60.0, avg_mpg=6.5  # Very high cost
        )

        # Cost: max(0, 100 - ((10 - 2.26)/2.26)*100) = max(0, -242) = 0
        # Util: 100
        # MPG: 100
        # Score: 0*0.35 + 100*0.35 + 100*0.30 = 65

        assert score == 65, f"Expected score 65 with high cost, got {score}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTEGRATION TESTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class TestDriverScorecardIntegration:
    """Integration tests for driver scorecard endpoint"""

    @patch("database_mysql.get_sqlalchemy_engine")
    def test_scorecard_returns_total_records_field(self, mock_engine):
        """Driver scorecard should include total_records in metrics"""
        from database_mysql import get_driver_scorecard

        # Mock the database results
        mock_conn = MagicMock()
        mock_engine.return_value.__enter__ = MagicMock(return_value=mock_conn)
        mock_engine.return_value.__exit__ = MagicMock(return_value=False)

        # Create mock row with correct field order
        # row[0]=truck_id, [1]=optimal_speed_count, [2]=total_moving_count, [3]=avg_speed,
        # [4]=max_speed, [5]=optimal_rpm_count, [6]=total_rpm_count, [7]=avg_rpm,
        # [8]=idle_count, [9]=idle_consumption_sum, [10]=avg_consumption, [11]=consumption_stddev,
        # [12]=avg_mpg, [13]=best_mpg, [14]=worst_mpg, [15]=total_records, [16]=total_miles
        mock_row = (
            "TRK-TEST",  # 0: truck_id
            50,  # 1: optimal_speed_count
            100,  # 2: total_moving_count
            60.0,  # 3: avg_speed
            72.0,  # 4: max_speed
            80,  # 5: optimal_rpm_count
            100,  # 6: total_rpm_count
            1400,  # 7: avg_rpm
            10,  # 8: idle_count
            5.0,  # 9: idle_consumption_sum
            2.5,  # 10: avg_consumption
            0.5,  # 11: consumption_stddev
            6.2,  # 12: avg_mpg
            7.5,  # 13: best_mpg
            5.0,  # 14: worst_mpg
            200,  # 15: total_records
            450.0,  # 16: total_miles
        )

        mock_conn.execute.return_value.fetchall.return_value = [mock_row]
        mock_engine.return_value.connect.return_value.__enter__ = MagicMock(
            return_value=mock_conn
        )
        mock_engine.return_value.connect.return_value.__exit__ = MagicMock(
            return_value=False
        )

        # The actual function would need proper mocking
        # This is a structural test to verify the field is included
        assert True  # Placeholder - actual integration test would call the function


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
